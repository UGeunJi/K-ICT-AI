# Neural Network 신경망 모형

1943년 Warren McCulloch and Walter Pitts은 인간의 뇌를 수학적 모형으로 표현하여 인간처럼 판단을 수행하고자 하는 아이디어에서 신경망 모형을 
구성하기 시작했다.

$$이 ~~ 아이디어는 ~~ f(\displaystyle\sum_{}{}{w_i x_i + b})의 ~~ 값이 ~~ y ~~ 값이 ~~ 되도록 ~~ 미지수 ~~ w값을 ~~ 구하고자 ~~ 하는 ~~ 것이다.$$

![image](https://user-images.githubusercontent.com/84713532/233746415-da17ffea-2d85-4bfd-8093-8dee6d10e835.png)

$$Y = S(w_1 x_1 + w_2 x_2 + w_3 x_3 + b) = \frac{1}{1 + exp(-(w_1 x_1 + w_2 x_2 + w_3 x_3 + b))}$$

- Logistic Regression의 모형과 일치한다.

---

## OR 신경망 모형

입력값 x들을 OR 연산 방식으로 weight에 0과 1을 곱해주며 Y값을 구하고, OR 연산의 결과값과의 차이인 error를 구해냄

![image](https://user-images.githubusercontent.com/84713532/233747113-790897fb-612a-41b1-ad6c-50a5777c6348.png)

#### error를 줄이기 위한 방법

- 입력인 x값에 새로운 값을 입력해줌
- Weight = Weight - lambda X 기울기방향
- 그럼 새로운 입력값이 생기고 새로운 결과값을 통해 error를 계산할 수 있게 됨
- 이때 그래프가 Convex하다면 error 값은 반드시 줄어들게 됨

#### 식 - Weight = Weight - lambda X 기울기방향

![image](https://user-images.githubusercontent.com/84713532/233747435-7c8ec331-6720-4530-8112-a96c256a38d1.png)

--- 

## XOR 신경망 모형

- Problem: AND와 OR 연산만으로는 해결할 수 없는 값이 존재

![image](https://user-images.githubusercontent.com/84713532/233748266-27d406b1-f5e8-4432-8550-058bdd168544.png)

- Solution: XOR 연산을 응용

![image](https://user-images.githubusercontent.com/84713532/233748276-e3608660-8093-4a90-938b-09a695391776.png)


![image](https://user-images.githubusercontent.com/84713532/233749451-6f396266-485b-43da-8c7e-30f0256787bf.png)

```
if h1 = 9, h2 = -10, bias = -4,

0 X (-9) + 0 X (-10) - 4 = -4 --- S(-4) -> 0
1 X 9 + 0 X (-10) - 4 = 5 --- S(5) -> 1
1 X 9 + 0 X (-10) - 4 = 5 --- S(5) -> 1
1 X 9 + 1 X (-10) - 4 = -5 --- S(-5) -> 0
```

- x는 h를 구성하는 데에만 기여하고 output에는 직접적으로 기여하지 않음

![image](https://user-images.githubusercontent.com/84713532/233749659-f75361fe-c271-4c3b-b98c-c319b2456777.png)


#### 이 구조를 유지하며 층을 추가하여 깊어지는 것을 딥러닝이라고 한다.

하지만 층이 깊어지며 weight와 bias를 하나하나 구하지 못하게 됨

이 문제를 해결하는 Backpropagation이라는 계산법이 나옴

---

## Backpropagation

![image](https://user-images.githubusercontent.com/84713532/233750102-1a4e0f79-67c6-49c1-a710-19ecfcddbe2b.png)

<br>

$$f_n(h_{n-1}; w_n, ~~ b_n) = a(W_n h_{n-1} + b_n) ~~~~~ h_n = f_n (\theta_n)$$

<br>

![image](https://user-images.githubusercontent.com/84713532/233750132-5c4c53d0-f381-4055-90fa-13013daa394c.png)

![image](https://user-images.githubusercontent.com/84713532/233750379-a5e98654-c2f0-4b7c-81a4-4c66018774b1.png)

![image](https://user-images.githubusercontent.com/84713532/233750355-9fc9f5a5-fe1f-4d54-9ed7-34d4abc59540.png)

#### 이 과정을 반복하면서 weight의 값이 바뀌는데 Convex하다면 error는 점점 줄어든다.

#### 많이 반복할수록 좋고 이 과정을 빠르게 반복하기 위해 GPU를 사용하는데, 주로 엔비디아에서 생산되는 제품을 사용한다.

- 하지만 층이 깊어지며 Vanishing Gradient 문제가 발생한다.

---

## Vanishing Gradient(NN winter 2: 1986 ~ 2006)

#### Backpropagation을 반복하며 기울기가 0으로 수렴하게 되는 문제

$$W_i = W_i - t(h_{i-1})\lambda \delta_i, ~~ \delta_i = \delta_{i+1} t(W_{i+1}) h_i (1-h_i)$$

- Sigmoid 함수에서는 h값이 0~1 사이에서 나오기 때문에 h(1-h) 식에 의해 기울기는 점점 작아질 수 밖에 없음

이 문제 때문에 당시엔 Neural Network는 가망없는 기술이라고 생각하게 됨
하지만 2006년에 이 문제를 해결하며 활성화되는데 이전의 안 좋은 이미지를 바꾸고자 이름도 Neural Network에서 Deep Learning으로 바뀌게 됨

### Solution 1

$$ReLU(x) =
\begin{cases}
x, ~~ x > 0 \\
0, ~~ x ≤ 0
\end{cases}$$

#### 장점

- Sigmoid 함수의 복잡한 식을 노드마다 backpropagation, forwardpropagation을 반복하며 계산해야 하는데 이 과정을 간단하게 만들어서 시간을 단축시킴
- 비선형이면서 기울기가 0이 되지 않음

![image](https://user-images.githubusercontent.com/84713532/233751681-eb90448c-8892-49be-a4e0-a7e47ed91416.png)

### Solution 2

#### Dropout: 레이어마다 어떤 노드를 죽일 것인지를 랜덤하게 결정한다.

![image](https://user-images.githubusercontent.com/84713532/233751343-2ef7a65c-f3f9-4d16-8389-70868d77174f.png)

---

## SGD(Stochastic Gradient Descending)

- 빠르게 해에 수렴할 수 있도록 해주는 경사하강법의 일종이다.
- Cost 함수의 도함수를 계산하려면 모든 학습 데이터를 사용하고, 이를 통해 weight가 업데이트 되는데 일부 데이터를 사용하여 갱신하는 방법이 SGD이다.

<br>
<br>

$$w = w - \lambda \frac{\partial}{\partial w} Cost(w, b)$$

![image](https://user-images.githubusercontent.com/84713532/233752169-0cb75f04-3121-42fa-a9d2-c86784de0aef.png)

$$\frac{\partial}{\partial w} Cost(w, b) = - \displaystyle\sum_{i=1}{n}{[y_i - S(w x_i + b)] x_i}$$

![image](https://user-images.githubusercontent.com/84713532/233752173-274d6d8e-8c89-4d31-b6b0-9b5c6c4d847b.png)

- problem: 일부 데이터만을 사용하기에 Convex하지 않고, error가 순간적으로 증가할 수도 있음
- 이때 error가 증가하지 않도록 적절한 n의 값을 넣어서 계산해야 함

- SGD의 장점은 보존하되 단점으로 여겨지는 Cost 감소 불안정 이슈를 해결하는 여러 알고리즘이 제안되었다.
- SGD는 패스의 관성을 이용하는 momentum 방법과 학습율을 조정하는 Adagrad 방식으로 구분되고 이 둘을 합친 Adam으로 귀결된다.

![image](https://user-images.githubusercontent.com/84713532/233752373-40684856-76c8-483b-981c-d37452bd664d.png)
