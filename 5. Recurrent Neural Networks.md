# 시계열 모형

#### RNN은 기존의 신경망 모형을 확장하여 Time Series Forecasting이 가능하도록 만든 모형이다.

- 예를 들어, 현재 내 모습은 "과거의 내 모습 + 현재의 상황"에 의해 만들어진다는 상식적인 접근이다.
- 이렇게 시간에 따라 변하는 데이터를 시계열 데이터라고 하고 순서가 중요하며 간격이 있어야 한다.

### 시계열을 설명하는 가장 간단한 모형은 아래와 같다.

- 아래 첫 번째 모형은 AR(1) 모형이라고 한다. AR은 Autoregressive의 약어다.
- 두 번째 모형은 ARMA(1, 1) 모형이다.
- 통계학에서 아래와 같은 형태를 확장하여 ARIMA 모형이라고 하는데 이는 선형모형임을 알 수 있다.

$$Y_t = \phi Y_{t-1} + \epsilon_t$$

$$Y_t = \phi Y_{t-1} - \theta{\epsilon_{t-1}} + \epsilon_t$$

# RNN

- Simple RNN 모형

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/854b3ec4-351d-4f2b-995c-05db835e9d70)

- 모형은 입력과 히든 레이어 그리고 출력으로 나눠진다. 히든 레이어가 존재하고 비선형 활성함수를 사용한 비선형 모형이다.
- 그림에서 순환하는 모양의 화살표가 있는데 이 때문에 순환(Recurrent) 신경망이라고 부른다.
- 순환 과정은 우측 그림과 같이 펼칠 수 있다. 즉, t 시점의 h는 t 시점의 x와 t-1 시점의 h를 만든다.

$$여기서 ~~ h_t는 ~~ 정답 ~ y_t의 ~~ 차이를 ~~ Cost ~ 함수로 ~~ 만들어 ~~ 이를 ~~ 최소화하는 ~~ Weight를 ~~ 만들어주면 ~~ RNN ~ 모형이 ~~ 완성되는 ~~ 것이다.$$

$$y_t에 ~~ 대한 ~~ 추정은 ~~ 아래의 ~~ 식으로 ~~ 이루어진다.$$

$$h_t = f(h_{t-1}, ~ x_t) = tanh(W_{hh}h{t-1} + W_{xh}x_t)$$

$$y_t = W_{hy}h_t

활성함수 f()는 tanh or ReLU 함수를 사용한다.

### RNN의 다양한 변신

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/8679fd11-87f4-43c3-a9ae-bf3a4f12999d)

- RNN에서 Hidden Layer를 여러 개 사용하면 Multiple Layer RNN이 된다.

# char RNN

#### "hello"라는 문장을 학습시키는 간단한 character RNN model을 예로 들어보자. 이 모형은 h->e->l->l->o 의 sequence이다.

즉, h가 입력되면 e가 나오고 e가 입력되면 l이 나오도록 학습하는 것이다. 이를 위해 입력과 출력 단어를 dictionary로 구성해보자. 즉, 이 사전에는 h, e, l, o 4개의 문자만 존재한다.
이를 one hot encoding으로 표시하면 아래와 같다.

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/27131295-4f04-4018-8dab-f6a1939272c9)

- character language model은 위와 같이 문자를 수치벡터로 치환한다. 그러므로 문자 입력과 문자 출력을 학습하면 character RNN이 되고, 단어 입력과 단어 출력을 학습하면 word RNN이 된다.
- 한글도 수치변환을 하면 동일한 원리도 가능하다.

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/de2dd46d-377f-4a96-94b7-bfbd64abf7ac)

$$h_t = f(h_{t-1}, x_t) = tanh(W_{hh} h_{t-1} + W_{xh} x_t)$$

$$(3, ~ 3) * (3, ~ 1) + (3, ~ 4) * (4, 1)$$

- 입력 벡터 x는 4 * 1이고, W_xh는 3 * 4 matrix이다. 즉, hidden Size * input size의 크기를 가진다.
- 여기서, hidden Size는 마음대로 정하면 된다. "h"는 3 * 1 matrix이고, W_hh는 3 * 3(hidden Size * hidden Size)의 크기이다.
- 그러므로 히든 레이어는 3 * 1 matrix가 된다.

- W_hy는 output Size * hidden Size matrix로 이 역시 초기값은 난수로 설정한다. 이 예에서는 4 * 3 matrix이다.
- Hidden Layer의 값에 가중값을 곱해 생성된 output layer값을 아래의 식으로 계산한다.
- input으로 target에 해당하는 one-hot vector를 맞추도록 가중값을 조정하는 것이다.

$$y_t = W_{hy} h_t$$

$$(4, ~ 3) * (3, ~ 1) = (4, ~ 1)$$

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/fa995337-4f2f-4403-aeb1-499ab016f3fc)

#### 아래의 프로그램은 h가 입력되었을 때, hidden layer의 값(outputs)를 계산하는 예다.

return_sequence는 과거 히든 값에 대한 출력 여부를 지정하는 명령이다.
return_state는 마지막 히든값에 대한 출력 여부를 지정하는 명령이다.

```py
import numpy as np
from tensorflow.keras import layers
# One hot encodeing for each char in 'hello'
h = [1, 0, 0, 0]
e = [1, 0, 0, 0]
l = [1, 0, 0, 0]
o = [1, 0, 0, 0]

# One cell RNN input_dim (4) -> output_dim (3)
x_data = np.array([h]), dtype=np.float32)
hidden_size = 3
rnn = layers.simpleRNN(units=hidden_size, return_sequences=True,
                       return_state=True)
outputs, states = rnn(x_data)

print(x_data, x_data.shape)
print(outputs, outputs.shape)
```

#### 아래는 hell을 순차적으로 입력하였을 때 레이어으이 값을 계산하는 코드다.

여기서, outputs는 히든 레이어의 모든 sequence에 대해 값을 기억하거 있고, state는 마지막 sequence 히든 레이어 값만 가지고 있다.

```py
# One cell RNN input_dim (4) -> output_dim (3). sequence: 5
x_data = np.array([[h, e, l, l, o]], dtype=np.float32)
print(x_data, x_data.shape)

hidden_size = 3
rnn = layers.SimpleRNN(units=hidden_size, return sequences=True,
      return_state=True)
outputs, states = rnn(s_data)

pritn(outputs, outputs.shape)
print(states, states.shape)
```

#### 아래의 코드는 hello를 char 단위로 학습시켜 h -> e, e -> l, l -> l, l -> o 를 출력하는 프로그램이다.

```py
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
```

```py
text = "hello"
```

```py
char_vocab = sorted(list(set(text)))
vocab_size = len(char_vocab)
print(char_vocab)
print('글자 집합의 크기: {}'.format(vocab_size))
```

```py
char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여
print(char_to_index)
```

#### "hell" -> "ello"로 학습하기 위해 "hell"을 [[1, 0, 2, 2]]로 바꾸고, "ello"를 [[0, 2, 2, 3]]으로 바꾼다. 이후, one-hot 변환을 수행하고 return_sequences=True, TimeDistributed 레이어를 사용하여 cost를 many2many 방식으로 계산하도록 한다.

```py
_tep = []
for ch in text:
    _tmp.append(char_to_index[ch])
_tmp
```

```py
train_X = [_tmp[:4]]
train_y = [_tmp[1:]]
train_X, train_y
```

```py
seq_length = 4
n_samples = 1
```

```py
print(train_X, train_y)
```

```py
train_X = to_categorical(train_X)
train_y = to_categorical(train_y)
```

```py
input_size = train_X.shape[2]   # 입력과 출력의 one hot 벡터 크기
output_size = train_y.shape[2]
```

```py
# hidden node 크기는 마음대로 정할 수 있다.
n_hidden = 5
model = Sequential()
model.add(layers.SimpleRNN(n_hidden, input_shape=(None, input_size), return_sequences=True))
model.add(layers.TimeDistributed(layers.Dense(output_size, activation='softmax'))
```

아래는 TimeDistributed를 설명하는 그림으로 many2many 방식으로 출력이 나오므로 cost 함수를 계산할 때, 시간에 따른 모든 출력노드에서 cost를 계산한다.
TimeDistributed 레이어를 사용하지 않으면 many to one 방식의 cost를 계산한다.

- TimeDistributed

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/c8265192-01b6-4d87-ac1c-bcc86f172592)

학습된 모형을 이용해 "hell"을 입력으로 예측한 결과, [0, 2, 2, 3] 즉 "ello"가 출력되었다.

```py
pred = model.predict(train_X)
pred
```

```py
np.argmax(pred, 2)
```

# Many to One Model

Many to one RNN은 문장을 판별하는 문제로 예를 들어, 영화평을 입력하면 긍부정을 판단하는 것이다. 문장은 tokenization을 통해 단어들의 모음으로 변환되고 이는 다시 one-hot encoding을 통해 수치로 변환된다.

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/e717d061-2c8c-4b0e-a0ea-38196845f5b9)

#### 아래와 같이 문자 단위로 g -> o -> o -> d를 입력하면 1을 출력하고, b -> a -> d는 0을 출력하도록 RNN 모형을 만들어보자.

```py
words = ['good', 'bad', 'worse', 'so good']
y_data = [[1], [0], [0], [1]]
```

words의 모든 문자를 join하고 set에 저장한 후, 다시 리스트엥 저장한다. 이후, <pad>라는 "0"에 해당하는 문자를 삽입한다. 이후, index --> character, character -> index로 변환하는 딕셔너리를 생성한다.
  
```py
# creating a token dictionary
char_set = ['<pad>'] + sorted(list(set(''.join(words))))
idx2char = {idx : char for idx, char in enumerate(char_set)}
char2idx = {char : idx for idx, char in enumerate(char_set)}
  
print(char_set)
print(idx2char)
print(char2idx)
```
  
### Many to One model

- "good"는 [6, 7, 7, 4]가 되고, "bad"는 [3, 2, 4]이 된다.

```py
# converting sequence of tokens to seqeunce of indices
x_data = list(map(lambda word : [char2idx.get(char) for char in word], words))
x_data_len = list(map(lambda word : len(word), x_data))
  
print(x_data)
pritn(x_data_len)
```
  
- x_data으이 길이가 다 다르다. 이 문제를 해결하기 위해 padding을 사용한다.
  
```py
max_sequence = 10
x_data = pad_sequences(sequences = x_data, maxlen = max_sequence,
                       padding = 'post', truncating = 'post')
y_data = np.array(y_data)
# checking data
print(x_data)
print(y_data)
```

### 이 밖에 Word Embedding, LSTM((Long Short Term Memory), stacked RNN, Time Series Forecasting, IMDB Pos/Neg 판별, Transformer, BERT(Bidirectional Encoder Representations from Transformers), GPT(Generative Pre-trained Transformer)도 있다.
