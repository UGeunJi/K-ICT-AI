 ## Linear Regression의 한계
 
 예측하고자 하는 종속변수가 Binary일 경우, <br>
 예를 들어 Y=0은 정상 제품, Y=1은 불량 제품을 나타내고, X = (1, 2, 1)은 부품의 속성을 나타내는 독립변수라고 하자. <br>
 아래의 Data에서는 X = (1, 2, 1)일 때, Y=0이므로 정상 제품임을 의미한다.
 
 $$X = \begin{pmatrix} 1&2&1 \\
                   1&3&2 \\
                   1&3&4 \\ 
                   1&5&5 \\ 
                   1&7&5 \\ 
                   1&2&5 \\ \end{pmatrix}, ~~ Y = \begin{pmatrix} 0 \\
                                                                  0 \\
                                                                  0 \\ 
                                                                  1 \\
                                                                  1 \\
                                                                  1 \\ \end{pmatrix}, ~~ \beta \begin{pmatrix} \beta_0 \\
                                                                                                               \beta_1 \\
                                                                                                               \beta_2 \\ \end{pmatrix}, ~~~~~~ 여기서, ~~ \beta는 미지수임$$
                                                                                                               
우리의 목표는 베타들을 구하고 X=(1, 2, 4)와 같이 새로운 제품이 들어왔을 때, 이 제품이 불량인지 아닌지 로지스틱 회귀모형을 통해 판단해보는 것이다.

$$이 ~~ 경우, ~~ 일반적 ~~ 회귀결과, ~~ \hat{y}의 ~~ 치역은 ~~ (-\infty, \infty)가 ~~ 된다. ~~ 우리가 ~~ 원하는 ~~ \hat{y}은 ~~ 0과 ~~ 1이므로 ~~ 적절치 ~~ 않다.$$
                                            
                                            
![image](https://user-images.githubusercontent.com/84713532/233281536-a894195f-18ae-469d-a898-6430cdba9b5d.png)
                                            
---

## Logistic Regression

#### 이 문제를 해결하기 위해 로지스틱 함수를 사용한다.

로지스틱 함수는 0~1 사이의 값을 가지는 함수로 S 혹은 Sigma 모양과 비슷하여 Sigmoid 함수라고도 한다. <br>
우변에 Logistic 함수를 적용하여 양변의 스케일 문제를 해결할 수 있다. <br>
이처럼 Logistic 함수를 이용하여 회귀분석을 수행하는 것이 Logistic Regression(Cox, 1958)이라고 부른다.


![image](https://user-images.githubusercontent.com/84713532/233282619-7ebac8d4-f1ba-4bb1-a394-c3ad9e720723.png)

$$0 ~~ < ~~ Y = \frac{1}{1 + e^{-x}} ~~ < ~~ 1$$

<br>

$$ y = \frac{1}{1 + exp(-\beta_0 - \beta_1 x_1 - \cdots - \beta_k x_k)}$$

<br>

$$log\Big(\frac{p}{1 - p}\Big) = X \cdot \beta = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k ~~~~~~ p = Pr(Y = 1)$$

---

## Cross Entropy Loss Function

#### 최소 Cost를 찾아서 계속 a 값을 바꿔줘야 한다. 수월하게 찾기 위해 Cross Entropy Loss Function을 활용한다. <br>
b는 Cost 함수를 최소화하는 베타 값이다. <br>
Cost 함수는 정답이면 작아지고, 오답이면 커지는 함수이면서 동시에 Convex(매끈)해야한다.

![image](https://user-images.githubusercontent.com/84713532/233286643-dfdde7ab-c016-4497-bb37-e524a3923440.png)

아래 Cross Entropy 함수는 Convex하면서 비용함수의 성질을 만족한다. 여기서, 베타 대신 W를 사용하자. <br>
T는 우리가 맞춰야 할 값이라고 하면

![image](https://user-images.githubusercontent.com/84713532/233287475-37974a22-62d8-4f69-8515-bd6791ecde30.png)

$$T = 1일 ~~ 때, ~~ Y = 1이면 ~~ cost = 0, ~~ Y = 0이면 ~~ cost = \infty$$

$$T = 0일 ~~ 때, ~~ Y = 1이면 ~~ cost = \infty, ~~ Y = 0이면 ~~ cost = 0$$

#### Convex Function에서 기울기가 0인 지점을 찾아가기 위해 임의의 초기값 W에서 기울기 방향으로 조금씩 (learning rate: a) 움직여 가는 방법을 사용한다.
#### 즉, 임의의 초기값에서 출발하여 기울기 방향으로 움직이면 언젠가는 기울기가 0인 지점에 도착한다는 원리다.

![image](https://user-images.githubusercontent.com/84713532/233288766-cce7f97b-4577-4aaf-866b-f5f888087b6f.png)

---

## Gredient Descent Algorithm

![image](https://user-images.githubusercontent.com/84713532/233289163-29eeefab-c679-4234-a628-5a5556fc53c7.png)

$$x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}$$

$$x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}$$
                                                                                                               
$$\eta: ~~ learning ~~ rate$$

$$\frac{\partial f}{\partial x_0}: ~~ 기울기$$

- 기울기 반대 방향으로 찾아가야 하기에 부호가 minus(-)인 것이다.

### Local Minimum & Global Minimum

- Convex하지 않을 때
   - Local Minimum: 전체적으로 Cost가 최소가 아닌 부분적으로 최소가 되는 부분
   - Global Minimum: 전체적으로 Cost가 최소가 되는 부분

$$Cost(W, b) = \frac{1}{m} \displaystyle\sum_{i=1}^{m}{(H(x^{(i)})-y^{(i)})^2}$$

![image](https://user-images.githubusercontent.com/84713532/233290818-460965b8-c22e-4a4f-86f5-1caba739fb92.png)

- 통계학에서의 parameter: beta
- 머신러닝에서의 parameter: weight

<br>

$$Cost(W) = \displaystyle\sum_{}{}{-T_i log(Y_i) - (1 - T_i) log(1 - Y_i)}$$

$$W: = W - \alpha \frac{\partial}{\partial W} Cost(W)$$

```
Gradient Descent Algorithm은 초기값에서 출발하여 1 step애 Learning Rate 속도로 이동하여 해로 다가가는 알고리즘이다.
일반적으로 초기값은 난수로 지정하고 learning rate는 대강 지정한 다음, 예를 들어 10,000회 이동하라는 명령을 내린다.
이때, 두 가지 경우가 나타날 수 있다.
1. 초기값이 해로부터 너무 멀면 주어진 반복 내에서 수렴하지 못 할 수 있다.
2. learning rate가 너무 크면 해를 지나쳐버릴 수 있고, 너무 작으면 도달하지 못 할 수도 있다.
```

![image](https://user-images.githubusercontent.com/84713532/233293164-fe947757-f911-45ac-8784-2df3650d3844.png)

### Underfitting & Overfitting

![image](https://user-images.githubusercontent.com/84713532/233293468-8943531f-f671-4a64-affb-3e4b46887798.png)

- 선형 회귀는 Underfitting되는 경우가 많고, 딥러닝은 Overfitting되는 경우가 많다.

---

## MNIST Data

```py
Mnist_Softmax_TF2.ipynb
import tensorflow as tf
import numpy as np
from tensorflow.keras import datasets
from tensorflow.keras.utils import to_categorical

mnist = dataset.mnist
(train_x, train_y), (test_x, test_y) = mnist.load_data()
train_x = train_x.reshape(-1, 784) # pixel 개수 28X28
test_x = test_x.reshape(-1, 784)
train_x = train_x / 255
test_x = test_x / 255
train_y_onehot = to_categorical(train_y)
test_y_onehot = to_categorical(test_y)
```

```py
from tensorflow.keras import layers

model = tf.keras.Sequential()
model.add(layers.Dense(10, activation='softmax', input_dim=784))
model.compile(optimizer='sgd', loss = 'categorical_crossentropy', metrics=['accuracy'])
model.fit(train_x, train_y_onehot, batch_size = 100, epochs=5)
model.evaluate(test_x, test_y_onehot)
```
