# Auto-Encoder

- Auto Encoder는 Input과 Output이 같은 딥러닝 모형임
- Target이 없으므로 Unsupervised Learning 형태임
- Input이 히든 레이어를 통과했다가 다시 재현되므로 암호화, 복호화가 일어나는 것임
- 일종의 통계학의 PCA(Principal Component Analysis)에 대한 비선형 버전으로 볼 수 있음
- 아래 그림에서 Code는 무엇일까? Code는 Input의 압축정보로 볼 수 있음
- Auto Encoder는 Noise Reduction, Anomaly Detection 등의 용도로 사용됨

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/96c412af-12b2-4f93-8993-4907d0ba5709)

## PCA

- PCA is a method of reducing dimensions by using the correlation of multidimensional variables.
- The goal of the PCA is to summarize the p-dimensional variables into k-dimensional variables (k < p) with minimal loss.

$$X \sim ( \mu, ~ \sum_{}{}{})$$

$$ \sum_{}{}{} = \Gamma \cdot D_\lambda \cdot \Gamma^`$$

$$ \Gamma ~ is ~ an ~ p ~ by ~ orthononal ~ matrix ~ composed ~ of ~ p ~ eigenvectors$$

$$ D_\lambda ~ is ~ an ~ p ~ by ~ p ~ diagonal ~ matrix ~ composed ~ of ~ p ~ eigenvalues$$

$$ PCA^\prime = \Gamma \cdot X^\prime$$

![image](https://github.com/UGeunJi/K-ICT_Analysis-Infra-Application_AI/assets/84713532/d3f5661f-9d98-44f2-83ee-f17771686315)

#### 아래는 PCA를 파이썬으로 구현한 코드이다.

```py
import numpy as np
from sklearn.decomposition import PCA
def standardScaler(X):
    return (X - np.mean(X, axis = 0)) / np.std(X, ddof = 1m axis = 0)
    
x = np.array([[1, 2, 3],
              [2, 4, 5],
              [3, 7, 6],
              [4, 7, 7]])
x_norm = standardScaler(x)

pca = PCA(n_components=3)
proj = pca.fit_transform(x_norm) # PCA Score
diag = np.diag(pca.explained_variance_) # eigen values
eVector = pca.components_.T # eigen vectors
cov = np.dot(np.dot(eVector, diag), eVector.T) # covariance 복원 확인
```

```py
from sklearn.datasets import load_digits
from matplotlib import pyplot as plt
digits = load_digits()
print(digits.data.shape) # digits는 (1797, 64) 데이터로 8 * 8 숫자 이미지다.

fig = plt.figure(figsize=(6, 6)) # figure size in inches
for i in range(64):
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], ytichs=[])
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    
from sklearn.decomposition import PCA
pca - PCA(n_components=3)
proj = pca.fit_transform(digits.data) # PCA Score
proj.shape  # prrj는 PCA를 통해 (1797, 3) 데이터로 64개의 차원을 3차원으로 줄인 데이터다.

plt.scatter(proj[:, 0], proj[:, 1], c=digits.target)
plt.colorbar()
```

```
좌측 그림은 8*8 숫자 이미지 모형이고 우측은 이를 PCA로 3차원 압축했을 때, 3차원 공간에서 숫자들(점들)이 군집화 된 모형을 보여주는 결과다.
군집화되어 있다는 것은 같은 숫자들을 3차원 공간에서 모여있음을 의미하는 것으로 64차원을 3차원으로 줄여도 어느 정도 숫자 분류가 가능함을 시사하는 결과다.
```

#### Auto-Encoder-Noise Reduction: 이미지에 노이즈 제거
#### Auto-Encoder-Colorization: 이미지 컬러화
#### Auto-Encoder-Image Search: 비슷한 이미지 찾기

---

# GAN(Generative Adversarial Network)

- GAN은 실제 데이터와 비슷한 확률분포를 갖는 허구 데이터(fake Data)를 생성함
- 예를 들어 필기체 글자를 학습하면 비슷하게 필기체 글자를 흉내내기도 하고, 사람 얼굴을 학습하면 가짜 사람 얼굴을 만드는 것도 가능함
- GAN에는 생성망과 판별망 두개의 딥러닝 네트워크가 존재함
- 생성망은 임의의 저차원 난수로부터 고차원 이미지를 생성하고 판별망은 생성된 이미지를 심사해서 진짜인지, 가짜인지 구별하는 네트워크임
- 생성망은 판별망을 속일 때까지 학습하고, 판별망은 생성망에 속지 않도록 학습함

#### Object Detection, 지문인식 기술

---

# 강화학습

- Action에 따른 보상 혹은 penalty를 통해 컴퓨터를 훈련하는 방법을 강화학습이라고 한다.
  - 주어진 Environment에서 Action을 수행하고 그 결과로 State가 변화한다.
  - Action이 끝나고 나면 Reward or Penalty가 주어진다.
  - Reward에 크기 혹은 존재 여부에 의해 현 상태의 Action 선택에 대한 의사결정 룰이 업데이트된다.
  - 랜덤하게 이 과정을 반복하여 의사결정 룰이 정확해지면 강화학습 룰이 완성되는 것이다.
