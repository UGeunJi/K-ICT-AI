# 빅데이터 개념의 시초

컴퓨터 입장에서는 데이터가 커져도 기술적으로 달라질 것이 없음
하지만 데이터 크기가 기술적, 물리적으로 단일 컴퓨터, 단일 데이터베이스로 처리하는데 무리가 되는 크기의 데이터가 있다면 or 비용 이슈가 있다면

비용 이슈는 초반엔 증가하는 데이터 양에 비례하여 선형적으로 비용도 증가하다가 어느 시점부터는 기하급수적으로 증가함

ex) 구글, 네이버 검색 정보, 유튜브의 보유 동영상, 페이스북 보유 자료, 센서 로그 등 대용량 컴퓨터로 처리하는 것이 비용 면에서 효율적이 않음

구글은 초기에 통신망이 안 좋았기에 사이트가 복잡하면 속도가 느려져서 간단한 형태가 됨

#### 이에 대한 해결책으로 분산 처리 Computing 방법론 탄생 - 구글이 개발

빅데이터 시대 초반에는 분산처리를 통해 막대한 양의 정보를 처리하여 부가가치를 창출하고자 함
-> 현재는 이러한 내용보다는 빅데이터와 머신러닝의 조합에서 부가가치를 창출하는 쪽으로 발전함

---

## 기계학습 Feature Engineering

#### 정형 데이터와 비정형 데이터

비정형 데이터: 로그 데이터, 이미지 데이터, 사운드 데이터

![image](https://user-images.githubusercontent.com/84713532/233253210-f8d4e33f-62b5-4324-b34c-16a8ebc5fa25.png)

암 존재 여부(y - Bernign / Malignant)를 판단하기 위해 여러 features(x)를 구함

$$ y = f(x) $$

- 과거에는 우측과 같이 이미지 정보를 요약해서 정형 데이터로 변환시켜 분석에 사용함
  - 데이터 손실이 일어남
  - 주요 feature만 추렸기 때문에 분석하기는 편함
- 현재에는 정형 데이터로 변환 후 분석하기도 하지만, 이미지 pixel 값을 데이터로 보고 그대로 분석에 사용하는 기법이 더 좋은 예측력을 보이고 있음
  - 위의 분석 방법을 발전된 computing power로 극복함
- 즉, 분석에 정형 데이터 뿐 아니라 비정형 신호 데이터까지 활용되고 있음

---

## Static / Event Log Data

#### Static Data: 개체의 속성에 해당하는 데이터로 시간에 따라 바뀌지 않는 데이터를 말함
- ex) 성별, 연령, 지역, 제조사, 생산일 등

#### Event Log Data: 개체의 상태에 해당하는 데이터로 시간에 따라 바뀌는 데이터를 말함
- ex) 현 위치, 조회 키워드, 클릭 페이지 등

일반적으로 행은 관측단위, 열은 변수로 지정되는 정형 데이터의 구조에서는 Event Log Data를 처리하기 어려움
하지만, 현재 Event Log Data를 활용한 데이터 처리 수요가 증가하고 있음
- ex) 구글, 페이스북 광고 추천, 페이스북 자살 예측, 센서 데이터에 의한 Alert 시스템 등

---

## 통계학

예시 문제: 우리나라 사람 중 키 185cm 이상인 사람의 비율은?

Solution 1: 1000명의 키를 검사하여 185cm 이상인 사람의 비율로 추정
Solution 2: 185cm 이상인 사람이 나올 때까지 검사하여 비율은 1/n으로 추정
Solution 3: 30명의 키를 측정하여 평균과 표준편차를 구해 정규분포 가정 하에 계산

- 가장 정확한 방법은? Solution 1
- 가장 효율적인 방법은? Solution 3

#### 비용과 정확성의 반비례 관계를 극복해야 한다.

![image](https://user-images.githubusercontent.com/84713532/233255008-4041cb55-42d0-4cdf-9c7a-dd31be8bd25c.png)

표본오차: 모집단에서 일부의 표본만을 선택해서 결과를 냈을 때 발생하는 오차
-> n을 높이면 오차는 줄어듦

But, 비표본 오차는 응답할 때 자신의 개인적인 정보나 성형을 감추기 위해 무응답이나 자신의 생각과 다른 응답을 할 수도 있음
Therefore, 표본오차를 줄이는 데에 집중함

- Small Data의 경우, 분석 정확도를 위해 모든 정보를 사용해 추정 혹은 의사결정을 수행함
- 분포 가정을 통해 재현성을 검정하고자 함
- 재현성: 다른 표본에서도 우리의 추정이나 의사결정이 재현됨을 증명하는 것
- Big Data의 경우는 풍부한 데이터 셋을 가지고 있기 때문에 재현성을 분포를 통해 증명하는 것이 아니고 실증적으로 재현이 가능함
  - 스몰 데이터인 통계학에서는 데이터가 작기 때문에 모든 데이터를 다 써서 선형분석을 함
  - 빅데이터인 딥러닝에서는 Training set과 Test Set을 나눠서 훈련시킨 모델을 가지고 test해서 재현성을 측정

#### -> Small Data와 Big Data 간의 차이를 위의 내용으로 정리할 수 있음
