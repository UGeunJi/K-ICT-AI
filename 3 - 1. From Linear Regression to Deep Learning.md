## Regression toward the mean

- 부모와 아이의 키의 상관관계

![image](https://user-images.githubusercontent.com/84713532/233258548-a5612b6e-0bb7-40d5-b648-31956f526ce6.png)

#### 우성 형질은 열성으로, 열성 형질은 우성으로 조금씩 평균을 향해 간다. -> 회귀 분석

$$y = ax + b$$

a와 b를 안다면, x인 부모의 키를 넣으면 y인 아이의 키를 알 수 있다.

$$y = f(x)$$

y: Output <br>
f: Name of Function <br>
x: Input 

---

## Linear Regression

![image](https://user-images.githubusercontent.com/84713532/233259199-8dd20793-ecfe-4ac2-b5a7-6a828c08a66a.png)

$$Y = \alpha + \beta x + \epsilon$$

$$SSE = \displaystyle\sum_{}{}{(\epsilon_i^2)} = \displaystyle\sum_{}{}{(Y_i - \alpha - \beta x_i)^2}$$

$$\epsilon = Y_i - \alpha - \beta x_i$$

#### SSE: Sum of Square Error

- 제곱으로 안하고 절댓값으로 해도 되지만, 제곱으로 하면 미분이나 적분하기 편함

#### 입실론은 y = a + bx인 선과 점 사이의 거리, 즉, 오차를 의미함

직선으로 예측했을 경우, 오차 입실론이 존재, 오차가 작을 수록 좋은 모형임

![image](https://user-images.githubusercontent.com/84713532/233263824-68f2e732-421f-45a7-8590-d7e9f94fb5e7.png)

#### SSE와 알파와의 관계를 위와 같이 2차 함수의 그래프로 나타낼 수 있고, 수많은 알파에 대해 기울기가 0이 되는 지점인 알파햇에서 SSE가 최소가 되는 것을 확인할 수 있다.

- Least Square Estimation for a, b

$$\frac{\partial SSE}{\partial \alpha} = 2\displaystyle\sum_{}{}{(Y_i - \alpha - \beta x_i)} (-1) = 0 ~~ \cdots \cdots \cdots ~~ (1)$$

$$\frac{\partial SSE}{\partial \beta} = 2\displaystyle\sum_{}{}{(Y_i - \alpha - \beta x_i)} (-x_i) = 0 ~~ \cdots \cdots \cdots ~~ (1)$$

<br>

$$(1) ~~ \overline{x}를 ~~ 곱하면,$$

<br>

$$\alpha \displaystyle\sum_{}{}{x_i} + \beta \overline{x} \displaystyle\sum_{}{}{x_i} = n \overline{x} \overline{Y} ~~ \cdots \cdots \cdots ~~ (3)$$

$$\alpha \displaystyle\sum_{}{}{x_i} + \beta \displaystyle\sum_{}{}{x_i^2} = \displaystyle\sum_{}{}{x_i Y_i} ~~ \cdots \cdots \cdots ~~ (4)$$

$$(4) - (3)을 ~~ 하면,$$

$$\hat{\beta} = \frac{\textstyle\sum_{}{}{x_i Y_i - n \overline{x} \overline{Y}}}{\textstyle\sum_{}{}{x_i^2 - n \overline{x}^2}} = \frac{\textstyle\sum_{}{}{(x_i - \overline{x})}(Y_i - \overline{Y})}{\textstyle\sum_{}{}{(x_i - \overline{x}^2)}}, ~~~~~ \hat{\alpha} = \overline{Y} - \hat{\beta} \overline{x}$$

#### 이때 베타햇을 기계학습에서는 학습이라고 하고, 통계학에서는 모수추정이라고 한다.
